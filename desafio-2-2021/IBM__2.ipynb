{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IBM_#2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdATwg98calG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93172137-5383-49c3-ff73-4aa87131e5a4"
      },
      "source": [
        "!pip install paho-mqtt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting paho-mqtt\n",
            "  Downloading paho-mqtt-1.6.1.tar.gz (99 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▎                            | 10 kB 22.9 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 20 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 30 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 40 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 51 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 61 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 71 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 81 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 92 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 99 kB 2.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: paho-mqtt\n",
            "  Building wheel for paho-mqtt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for paho-mqtt: filename=paho_mqtt-1.6.1-py3-none-any.whl size=62133 sha256=64c50cf28a5f3c89bfd259eb95ceefc830f5401c1bc224e09be5a5698e831f72\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/bf/ac/2b3f43f8c6fcd0f4ba5395397458c521eb0b52d33b574a5a40\n",
            "Successfully built paho-mqtt\n",
            "Installing collected packages: paho-mqtt\n",
            "Successfully installed paho-mqtt-1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_BJPqkhpNCD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2a58be8-4586-4f6a-a262-b6552a0fe78a"
      },
      "source": [
        "!pip install pandas-profiling -U"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas-profiling in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Collecting pandas-profiling\n",
            "  Downloading pandas_profiling-3.1.0-py2.py3-none-any.whl (261 kB)\n",
            "\u001b[K     |████████████████████████████████| 261 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: missingno>=0.4.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.5.0)\n",
            "Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.1.5)\n",
            "Collecting phik>=0.11.1\n",
            "  Downloading phik-0.12.0-cp37-cp37m-manylinux2010_x86_64.whl (675 kB)\n",
            "\u001b[K     |████████████████████████████████| 675 kB 42.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.4.1)\n",
            "Requirement already satisfied: seaborn>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.11.2)\n",
            "Requirement already satisfied: matplotlib>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (3.2.2)\n",
            "Collecting PyYAML>=5.0.0\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 46.3 MB/s \n",
            "\u001b[?25hCollecting joblib~=1.0.1\n",
            "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
            "\u001b[K     |████████████████████████████████| 303 kB 55.8 MB/s \n",
            "\u001b[?25hCollecting visions[type_image_path]==0.7.4\n",
            "  Downloading visions-0.7.4-py3-none-any.whl (102 kB)\n",
            "\u001b[K     |████████████████████████████████| 102 kB 12.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.19.5)\n",
            "Collecting multimethod>=1.4\n",
            "  Downloading multimethod-1.6-py3-none-any.whl (9.4 kB)\n",
            "Collecting requests>=2.24.0\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 873 kB/s \n",
            "\u001b[?25hCollecting tangled-up-in-unicode==0.1.0\n",
            "  Downloading tangled_up_in_unicode-0.1.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 32.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (2.11.3)\n",
            "Collecting pydantic>=1.8.1\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 30.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: markupsafe~=2.0.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (2.0.1)\n",
            "Collecting htmlmin>=0.1.12\n",
            "  Downloading htmlmin-0.1.12.tar.gz (19 kB)\n",
            "Requirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (21.2.0)\n",
            "Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (2.6.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (7.1.2)\n",
            "Collecting imagehash\n",
            "  Downloading ImageHash-4.2.1.tar.gz (812 kB)\n",
            "\u001b[K     |████████████████████████████████| 812 kB 34.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling) (3.0.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3->pandas-profiling) (2018.9)\n",
            "Collecting scipy>=1.4.1\n",
            "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.1 MB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic>=1.8.1->pandas-profiling) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.2.0->pandas-profiling) (1.15.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling) (2.0.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling) (2.10)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.7/dist-packages (from imagehash->visions[type_image_path]==0.7.4->pandas-profiling) (1.2.0)\n",
            "Building wheels for collected packages: htmlmin, imagehash\n",
            "  Building wheel for htmlmin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27098 sha256=5717d2da5187aaaa333d471c048a4d2bd4fbba668f83f81cb9368223aa19de39\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/e1/52/5b14d250ba868768823940c3229e9950d201a26d0bd3ee8655\n",
            "  Building wheel for imagehash (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imagehash: filename=ImageHash-4.2.1-py2.py3-none-any.whl size=295207 sha256=0fe9b0c40b547133897752e93d826ae5c37aba402e295d632f6d2756ea41ef79\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/d5/59/5e3e297533ddb09407769762985d134135064c6831e29a914e\n",
            "Successfully built htmlmin imagehash\n",
            "Installing collected packages: tangled-up-in-unicode, scipy, multimethod, visions, joblib, imagehash, requests, PyYAML, pydantic, phik, htmlmin, pandas-profiling\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.1.0\n",
            "    Uninstalling joblib-1.1.0:\n",
            "      Successfully uninstalled joblib-1.1.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: pandas-profiling\n",
            "    Found existing installation: pandas-profiling 1.4.1\n",
            "    Uninstalling pandas-profiling-1.4.1:\n",
            "      Successfully uninstalled pandas-profiling-1.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed PyYAML-6.0 htmlmin-0.1.12 imagehash-4.2.1 joblib-1.0.1 multimethod-1.6 pandas-profiling-3.1.0 phik-0.12.0 pydantic-1.8.2 requests-2.26.0 scipy-1.7.3 tangled-up-in-unicode-0.1.0 visions-0.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpDFjplQ1b2S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "374556aa-3676-4519-cc61-f6f7f5f5a250"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.14.1-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 8.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.26.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 22.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 30.8 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 495 kB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.0.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.2.1 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmNIHMNdsqLd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc163426-6bfa-456f-a766-d9e99c2632f0"
      },
      "source": [
        "!pip install feature_engine -U"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting feature_engine\n",
            "  Downloading feature_engine-1.1.2-py2.py3-none-any.whl (180 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 16.7 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 20 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 30 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 40 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 51 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 61 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 71 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 81 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 92 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 102 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 112 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 122 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 133 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 143 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 153 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 163 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 174 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 180 kB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.7/dist-packages (from feature_engine) (1.0.1)\n",
            "Collecting statsmodels>=0.11.1\n",
            "  Downloading statsmodels-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.8 MB 49.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.0.3 in /usr/local/lib/python3.7/dist-packages (from feature_engine) (1.1.5)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from feature_engine) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.7/dist-packages (from feature_engine) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.3->feature_engine) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.3->feature_engine) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0.3->feature_engine) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->feature_engine) (1.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->feature_engine) (3.0.0)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.11.1->feature_engine) (0.5.2)\n",
            "Installing collected packages: statsmodels, feature-engine\n",
            "  Attempting uninstall: statsmodels\n",
            "    Found existing installation: statsmodels 0.10.2\n",
            "    Uninstalling statsmodels-0.10.2:\n",
            "      Successfully uninstalled statsmodels-0.10.2\n",
            "Successfully installed feature-engine-1.1.2 statsmodels-0.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnTubkJAmnAj"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import csv\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import plotly.express as px\n",
        "import paho.mqtt.client as mqtt\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from pandas_profiling import ProfileReport\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.pipeline import make_pipeline, Pipeline\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
        "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
        "\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "from yellowbrick.regressor import ResidualsPlot\n",
        "\n",
        "from mlxtend.regressor import StackingCVRegressor\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "\n",
        "%matplotlib inline\n",
        "random_state = 42"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhRfiwbgcdhV"
      },
      "source": [
        "host = 'iot.maratona.dev'\n",
        "port = 31666\n",
        "username = 'maratoners'\n",
        "password = 'btc-2021'\n",
        "topic = 'quanam'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwEHuMsJjff8"
      },
      "source": [
        "def on_message(client, userdata, message):\n",
        "    global recieved_data\n",
        "    recieved_data = str(message.payload.decode(\"utf-8\"))\n",
        "    print(\"%s %s\" % (message.topic, message.payload))\n",
        "    # Write the returned messages into a csv file\n",
        "    csv_writer.writerow([recieved_data])\n",
        "    my_data_file.flush()   \n",
        "\n",
        "client = mqtt.Client(client_id='', \n",
        "                        clean_session=True, \n",
        "                        userdata=dict, \n",
        "                        protocol=mqtt.MQTTv311, \n",
        "                        transport='tcp')\n",
        "\n",
        "client.username_pw_set(username=username, password=password)\n",
        "\n",
        "client.connect(host=host, port=port)\n",
        "\n",
        "client.loop_start()\n",
        "client.subscribe(topic)\n",
        "\n",
        "my_data_file = open('quanam.csv', 'a+')\n",
        "csv_writer = csv.writer(my_data_file, delimiter=',')\n",
        "\n",
        "# Iterate over all elements until reach the first element one more time \n",
        "# and then stop the run of the cell manually\n",
        "while True:\n",
        "    client.on_message = on_message\n",
        "\n",
        "client.disconnect()\n",
        "client.loop_stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMCo5bvgrt41"
      },
      "source": [
        "with open('quanam.csv', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "    lines = [line.rstrip() for line in lines]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g28ZIIU3gFeC"
      },
      "source": [
        "splited_lines = []\n",
        "\n",
        "for line in tqdm(lines):\n",
        "    # Get values of each line\n",
        "    regex_found = re.findall('\"\"[A-Z]+[0-9]*\"\":\\s\"*\"*([0-9]+\\.*[0-9]*)\"*\"*', line)\n",
        "    # Assert that the right number of values is returned\n",
        "    assert len(regex_found) == 7 or line!='', f\"{len(regex_found)}\"\n",
        "    \n",
        "    splited_lines.append(np.array(regex_found).astype(np.float))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNO9T7cOjvNK"
      },
      "source": [
        "df = pd.DataFrame(splited_lines, columns=['ILLUM', 'HUMID', 'CO2', 'SOUND','TEMP','RYTHM', 'ID'])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjh-LsMYkZuj"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1B2pec-lga7"
      },
      "source": [
        "df = df.drop_duplicates()\n",
        "df = df.dropna()\n",
        "df = df.sort_values(by = 'ID').reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQDvvaYAlk9f"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MXr05Ionvgr"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbJo8RANnwwG"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzquwQhsnxiN"
      },
      "source": [
        "profile = ProfileReport(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVjkZ6fqpUPl"
      },
      "source": [
        "profile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PB__ebNrlQY"
      },
      "source": [
        "rcParams['figure.figsize'] = 14,8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynZirrSoJUPM"
      },
      "source": [
        "vars = df.drop(['df_index', 'ID'], axis=1).columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5Cd3zBTKKT5"
      },
      "source": [
        "for var in vars:\n",
        "    sns.regplot(data=df, x=\"RYTHM\", y=var)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8m6PYqsTn3l"
      },
      "source": [
        "correlation_matrix = df[vars].corr().round(2)\n",
        "figure = plt.figure(figsize=(12, 12))\n",
        "sns.heatmap(data=correlation_matrix, annot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfqmGVMVRQw3"
      },
      "source": [
        "sns.regplot(data=df, x=\"SOUND\", y='HUMID')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmpCASslsLhN"
      },
      "source": [
        "fig = plt.figure(figsize=(14, 8))\n",
        "ax = fig.add_subplot(111, projection = '3d')\n",
        "\n",
        "x = df['CO2']\n",
        "y = df['TEMP']\n",
        "z = df['RYTHM']\n",
        "\n",
        "ax.set_xlabel(\"CO2\")\n",
        "ax.set_ylabel(\"TEMP\")\n",
        "ax.set_zlabel(\"RYTHM\")\n",
        "\n",
        "ax.scatter(x, y, z)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwEA08w2UB2U"
      },
      "source": [
        "fig = px.scatter_3d(df, x='CO2', y='TEMP', z='RYTHM')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eu-ylF_1Kwxw"
      },
      "source": [
        "def check_distribution(df, variable):\n",
        "    plt.figure(figsize=(16, 4))\n",
        "\n",
        "    # histogram\n",
        "    plt.subplot(1, 3, 1)\n",
        "    sns.histplot(df[variable], bins=30)\n",
        "    plt.title('Histogram')\n",
        "\n",
        "    # Q-Q plot\n",
        "    plt.subplot(1, 3, 2)\n",
        "    stats.probplot(df[variable], dist=\"norm\", plot=plt)\n",
        "    plt.ylabel('Variable quantiles')\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxUFLQMkK84e"
      },
      "source": [
        "for var in vars:\n",
        "    check_distribution(df, var)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYj1m12_Le0C"
      },
      "source": [
        "# Check for outliers \n",
        "for var in vars:\n",
        "    sns.boxplot(y=df[var])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZxqR4q1wKLR"
      },
      "source": [
        "y_train = df['RYTHM']\n",
        "X_train = df.drop(['RYTHM', 'df_index', 'ID', 'HUMID'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sixNMf_lrrtY"
      },
      "source": [
        "X_train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQl_GLK0gb4V"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22OaJsTIbZwN"
      },
      "source": [
        "LR_pipe_scaler = Pipeline([\n",
        "    # feature Scaling - section 10\n",
        "    ('scaler', RobustScaler()),\n",
        "    \n",
        "    # regression\n",
        "    ('LR', LinearRegression())\n",
        "])\n",
        "\n",
        "lasso_pipe_scaler = Pipeline([\n",
        "    # feature Scaling - section 10\n",
        "    ('scaler', RobustScaler()),\n",
        "    \n",
        "    # regression\n",
        "    ('lasso', Lasso(alpha=0.0165, random_state=random_state))\n",
        "])\n",
        "\n",
        "ENet_pipe_scaler = Pipeline([\n",
        "    # feature Scaling - section 10\n",
        "    ('scaler', RobustScaler()),\n",
        "    \n",
        "    # regression\n",
        "    ('ENet', ElasticNet(alpha=0.0085, l1_ratio=0.1, random_state=random_state))\n",
        "])\n",
        "\n",
        "KRR_pipe_scaler = Pipeline([\n",
        "    # feature Scaling - section 10\n",
        "    ('scaler', RobustScaler()),\n",
        "    \n",
        "    # regression\n",
        "    ('krr', KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5))\n",
        "])\n",
        "\n",
        "model_lgb = Pipeline([\n",
        "    # feature Scaling - section 10\n",
        "    ('scaler', RobustScaler()),\n",
        "    \n",
        "    # regression\n",
        "    ('lgb', lgb.LGBMRegressor(objective='regression',\n",
        "                              learning_rate=0.1,\n",
        "                              max_depth=-1,\n",
        "                              n_estimators=200,\n",
        "                              num_leaves=2,\n",
        "                              random_state=random_state))\n",
        "])\n",
        "\n",
        "\n",
        "gbr = GradientBoostingRegressor(n_estimators=200, learning_rate=0.05,\n",
        "                                   max_depth=4, max_features='sqrt',\n",
        "                                   min_samples_leaf=15, min_samples_split=10, \n",
        "                                   loss='huber', random_state=random_state)\n",
        "\n",
        "xgboost = xgb.XGBRegressor(learning_rate=0.01, n_estimators=3000,\n",
        "                                     max_depth=3, min_child_weight=0,\n",
        "                                     gamma=0, subsample=0.7,\n",
        "                                     colsample_bytree=0.7,\n",
        "                                    objective='reg:squarederror',\n",
        "                                     scale_pos_weight=1, seed=27,\n",
        "                                     reg_alpha=0.00006, random_state=random_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2ha1243_2RK"
      },
      "source": [
        "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
        "    # Source: https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard\n",
        "    def __init__(self, base_models, meta_model, n_folds=5):\n",
        "        self.base_models = base_models\n",
        "        self.meta_model = meta_model\n",
        "        self.n_folds = n_folds\n",
        "   \n",
        "    # We again fit the data on clones of the original models\n",
        "    def fit(self, X, y):\n",
        "        self.base_models_ = [list() for x in self.base_models]\n",
        "        self.meta_model_ = clone(self.meta_model)\n",
        "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
        "        \n",
        "        # Train cloned base models then create out-of-fold predictions\n",
        "        # that are needed to train the cloned meta-model\n",
        "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
        "        for i, model in enumerate(self.base_models):\n",
        "            for train_index, holdout_index in kfold.split(X, y):\n",
        "                instance = clone(model)\n",
        "                self.base_models_[i].append(instance)\n",
        "                instance.fit(X[train_index], y[train_index])\n",
        "                y_pred = instance.predict(X[holdout_index])\n",
        "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
        "                \n",
        "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
        "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
        "        return self\n",
        "   \n",
        "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
        "    #meta-features for the final prediction which is done by the meta-model\n",
        "    def predict(self, X):\n",
        "        meta_features = np.column_stack([\n",
        "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
        "            for base_models in self.base_models_ ])\n",
        "        return self.meta_model_.predict(meta_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ml1GgKG_A6ne"
      },
      "source": [
        "stacked_averaged_models = StackingAveragedModels(base_models = (LR_pipe_scaler, lasso_pipe_scaler, ENet_pipe_scaler, KRR_pipe_scaler, model_lgb),\n",
        "                                                    meta_model = lasso_pipe_scaler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zpZ-TcFBCk6"
      },
      "source": [
        "for clf, label in zip([LR_pipe_scaler, lasso_pipe_scaler, ENet_pipe_scaler, KRR_pipe_scaler, model_lgb, xgboost, gbr, stacked_averaged_models], ['LR', 'Lasso', \n",
        "                                                'ENet', \n",
        "                                                'KRR', 'LGB', 'xgboost', 'gbr','Stack']):\n",
        "    scores = cross_val_score(clf, X_train.values, y_train.values, cv=5, scoring='r2')\n",
        "    print(\"R^2 Score: %0.6f (+/- %0.6f) [%s]\" % (\n",
        "        scores.mean(), scores.std(), label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQaANiNxBbqe"
      },
      "source": [
        "stacked_averaged_models.fit(X_train.values, y_train.values)\n",
        "stacked_train_pred = stacked_averaged_models.predict(X_train.values)\n",
        "print(r2_score(y_train, stacked_train_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1N26Kjxdc0i"
      },
      "source": [
        "answers_df = pd.read_csv('answers.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBCyNvoVd4fA"
      },
      "source": [
        "answers_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnUtXZDjd5RQ"
      },
      "source": [
        "profile = ProfileReport(answers_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LGHtiEWeHdQ"
      },
      "source": [
        "profile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFjxkvjxeH_w"
      },
      "source": [
        "ans_df_alt = answers_df.drop(['RYTHM', 'ID', 'HUMID'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4khgF77eip4"
      },
      "source": [
        "ans_df_alt.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLXlCSyjejtI"
      },
      "source": [
        "ans_pred = stacked_averaged_models.predict(ans_df_alt.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1vjC42gewPx"
      },
      "source": [
        "len(ans_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjWa-hrqexAR"
      },
      "source": [
        "my_answers_df = pd.read_csv('answers.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TO8hWySLh9-9"
      },
      "source": [
        "my_answers_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA42bo62hyR-"
      },
      "source": [
        "my_answers_df = my_answers_df.drop(['RYTHM'], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aehEE9NQiOiv"
      },
      "source": [
        "my_answers_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2rL8ly2h266"
      },
      "source": [
        "my_answers_df['RYTHM'] = ans_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb3atgIah58Z"
      },
      "source": [
        "my_answers_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geRrT2hkiRGx"
      },
      "source": [
        "my_answers_df.to_csv('MY_ANSWERS_DEF_2.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}